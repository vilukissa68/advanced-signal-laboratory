% !TEX encoding = UTF-8 Unicode
\documentclass[12pt,a4paper,english
% ,twoside,openright
]{tunithesis}

\special{papersize=210mm,297mm}

\author{Roosa Kuusivaara \& Väinö-Waltteri Granat}
\title{Image Recognition - Report} % primary title (for front page)
\thesistype{Laboratory Report} % or Bachelor of Science, Laboratory Report...

\usepackage{lastpage}
\usepackage[english]{babel}
\usepackage[
backend=biber,
style=authoryear,
citestyle=authoryear,
autocite=inline
]{biblatex}
\usepackage{csquotes}

\addbibresource{references.bib} %Imports bibliography file


\definecolor{tunipurple}{RGB}{78, 0, 142}

\newcommand\todo[1]{{\color{red}!!!TODO: #1}} % Remark text in braces appears in red
\newcommand{\angs}{\textsl{\AA}}              % , e.g. slanted symbol for Ångstöm
% Preparatory content ends here


\pagenumbering{roman} % was: {Roman}
\pagestyle{headings}
\begin{document}

% Special trick so that internal macros (denoted with @ in their name)
% can be used outside the cls file (e.g. \@author)
\makeatletter

% Create the title page.
% First the logo. Check its language.
\thispagestyle{empty}
\vspace*{-.5cm}\noindent

\begin{figure}
    \vspace{-1.3cm}
    \advance\leftskip-2.5cm
    \noindent\includegraphics{img/tunilogo.png}
\end{figure}
 
\vspace{2.5cm}
\begin{flushright}
\noindent\textsf{\LARGE{\@author}}

\noindent\vspace{0.5cm}

\noindent\Huge{\textsf{\textbf{\textcolor{tunipurple}{\@title}}}}
\end{flushright}
\vspace{13.7cm} % adjust to 12.7 this if thesis title needs two lines

% Last some additional info to the bottom-right corner
\begin{flushright}  
    \begin{spacing}{1.0}
      \textsf{Faculty of Information Technology and Communication Sciences (ITC)\\
      \@thesistype\\}
    \end{spacing}
\end{flushright}

% Leave the backside of title page empty in twoside mode
\if@twoside
\clearpage
\fi

% Turn off page numbering for the first pages
\pagenumbering{gobble}


% Some fields in abstract are automated, namely those with \@ (author,
% title, thesis type).
\chapter*{Abstract}
\begin{spacing}{1.0}
\noindent \@author: \@title\\
\@thesistype\\
Tampere University\\
Master’s Degree Programme in Signal Processing\\
October 2023 \\
\end{spacing}
\noindent\rule{12cm}{0.4pt}

\vspace{0.5cm}

% ---------------------------------------
% Abstract and keywords
% ---------------------------------------

\noindent
This report documents the work done in the Image Recognition assignment as a part of the Advanced Signal Processing Laboratory course. In the assignment we familiarize ourselves with modern machine learning, in particular deep learning, and apply them to the task of building a smile detector for real-time execution. The goal is to achieve an accuracy of at least $85 \%$ in classifying images based on facial expressions, smiles or non-smiles, using GENKI-4k dataset for training the network.


~

\noindent\textbf{Keywords:} Laboratory Report, Machine Learning, Deep Learning, Image Recognition


% Add the table of contents


\setcounter{tocdepth}{3}              % How many header level are included
\tableofcontents                      % Create TOC


% The actual text begins here and page numbering changes to 1,2...
% Leave the backside of title empty in twoside mode
\if@twoside
%\newpage
\cleardoublepage
\fi


\renewcommand{\chaptername}{} % This disables the prefix 'Chapter' or
                              % 'Luku' in page headers (in 'twoside'
                              % mode)


\chapter{Introduction}
\label{ch:intro}
In this report we describe our work done in the 'Image Recognition' laboratory assignment for the Advanced Signal Processing Laboratory. In this assignment we were to implement a system that would detect if a person was smiling or not from a live video feed, using Machine Learning approach, more specifically a Convolution Neural Network trained as a binary classifier.

The system consisted of two major modules. First a neural network which could classify smiling and non-smiling images with a minimum 85\% accuracy. The second module would capture live video feed from computers web camera, from which the module would capture a face from each frame. These frames would then be given for the network to classify if that captured face was smiling or not. The classification would then be shown in the programs UI to the user.

\pagenumbering{arabic}
\setcounter{page}{1} 
\section{Neural networks}
Neural networks are
Generally neural networks are trained using the gradien descdent algorithm.

\section{Face detection}


\chapter{Methodology}
\label{sec:methodology}
\section{Dataset}
For this assignment we were required to use the GENKI-4K dataset~\cite{genki}. GENKI-4K consists of 4000 images of faces, labeled either smiling or not smiling. This data set was to be randomly split into portions of 80:20 for training dataset and testing dataset.

To be able to input the GENKI-4K images into the neural network we resized the images to match the required 64x64 pixels size used by the network. The images were also normalized to values $0...1$. This is generally recommended to prevent issues with division and square root operations that would happend when using discrete integers.


\section{Base model implementation}
The described model was implemented as Pytorch~\cite{pytorch} model.

The base model didn't perform as well as was required by the assignment instructions, so we implemented multiple methods that are generally known to improve the accuracy of image classification models.

\section{Improved models}
Since the base model was a relatively small network, we decided to start optimizing accuracy by increasing the number of layers in the network. The basic idea was that by increasing the number of layers the network would be able to learn more detailed information and capture more of the latent features and thus be able to more accurately make predictions. The danger of increasing the size of the network is that each added parameters increases the training time and more importantly increases the prediction time. The increased prediction time could mean that our program would not be able to make predictions of real time video fast enough to be usable.

We ran a test were we trained models of different size with the same hyperparameters, to find what kind of layers would have the most benefit for the accuracy of the predictions. We noticed that by encreasing the the number of larger layers had more of a impact.

\section{Optimizing Hyperparameters}
The next step to increase the performance of the network was to optimize our hyperparameters. This is usually a difficult problem so we focused only on the following parameters: learning rate, number of epoch and batch size. The best choice for the hyperparameters is dependent on the network we decide to use, so we tested the base model and two of the best performing bigger models to find the optimal model and accompanying parameters.

We also experimented with two different optimizers, Adam and AdamW.
AdamW uses the same optimization algorithm as Adam, with the addition of dynamic learning rate (TODO:CONFIRM THIS!!!). Dynamic learning rate allows the optimizer to change the learning rate during training. In general we want to start with a high learning rate to find the area of local maximum fast and the use increasingly smaller learning rate to find the lowest loss. This should make the training faster and prediction a bit more accurate.

\section{Data augmentation}
\label{sec:dataaug}
Since the GENKI-4K dataset is a very small dataset in today's standards we used data augmentation to increase the amount of training data available. In the augmented dataset we included all the original images as such, plus 2 augmented images of each original images.

We used 3 different augmentations methods: flipping creates a mirror image relative to the y-axis, rotation rotates the image 90, 180 or 270 degrees, and finally color jitter changes the saturation of the images. The augmentations we applied at random during augmented dataset serialization and one augmented images be applied with 0 to 3 augmentations.

Finally we experimented with grayscale images. Greyscale images consist only of 1 channel pixels, where as color images use 3 channels. This means that neural network that takes only grayscale images has less parameters, and therefore faster predictions when compared to color images. Our hypothesis was that since smile should be classifiable from both grayscale images and color images equally well, the grayscale models might use the freed parameters to make more accurate predictions faster. We also created a grayscale version of the augmented dataset.

\subsection{Noise}
Adding noise to training images is a common technique to improve the training of image classification models. It differs from the previous data augmentation methods in the sense that it doesn't increase the size of the dataset, but prevents the network from seeing exactly the same images over and over again. Noise is added to images randomly with the goal of encouragin the model to explore the possibility space.

\chapter{Results}
\label{sec:results}
\section{Base Model}

\section{Larger Models}

To find a better model we experimented with larger models by adding non-downsampling blocks after the each side of downsampling block. The base models structure in our syntax is: 1x64 2x32 2x16 1x8. The last layer of each size is always a downsampling layers and the ones before that are non-downsampling layers. The extra layers should be able to capture more features which should enable for better classification. For this experimment we trained for 150 epochs to ensure that the larger models had enough time to learn.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Model} & \textbf{Accuracy}  \\ \hline
2x64 3x64 3x16 2x8 & 0.82 \\ \hline
3x64 4x64 4x16 3x8 & 0.66 \\ \hline
4x64 5x64 5x16 4x8 & 0.63 \\ \hline
2x64 2x64 2x16 1x8 & 0.80 \\ \hline
3x64 2x64 2x16 1x8 & 0.81 \\ \hline
1x64 4x64 4x16 1x8 & 0.83 \\ \hline
3x64 3x64 2x16 1x8 & 0.80 \\ \hline
\end{tabular}
\caption{Training accuracy and loss for different CNN models with Adam optimizer}
\label{tab:models}
\end{table}

Based on table results shown in table~\ref{tab:models} we can make an hypothesis that increasing the depth in the beginning of the network, where the layers are larger has greatly more effect to classification results.

\section{Hyperparameter Optimization}

\subsection{Learning rate}
We tested multiple learning rate for both adam and adamW optimizers. These results are shown in the table~\ref{tab:learningrates}. From the results we can see that the use of adamW didn't really benefit us, not in terms of training time or improved accuracy. This experiment helped us to better narrow down the optimal learning rate for our model.
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Learning Rate} & \textbf{Adam} & \textbf{AdamW} \\ \hline
0.2 & 0.62 & 0.55 \\ \hline
0.02 & 0.56 & 0.56 \\ \hline
0.002 & 0.58 & 0.55 \\ \hline
0.0002 & 0.79 & 0.76 \\ \hline
0.00002 & 0.62 & 0.60 \\ \hline
0.000002 & 0.56 & 0.54 \\ \hline
0.0000002 & 0.58 & 0.52 \\ \hline
0.00000002 & 0.79 & 0.51 \\ \hline
\end{tabular}
\caption{Training accuracy for Adam and AdamW optimizers with different learning rates}
\label{tab:learningrates}
\end{table}

We decided to do another experiment with learning rates closer to the values of $0.0002$ which was determined to be best magnitude in the previous experiment. These results are shown in the table~\ref{tab:learningrates2}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Learning Rate} & \textbf{Adam} & \textbf{AdamW} \\ \hline
0.00005 & 0.63 & 0.61 \\ \hline
0.0001 & 0.66 & 0.70 \\ \hline
0.0003 & 0.64 & 0.62 \\ \hline
0.0005 & 0.62 & 0.62 \\ \hline
0.0007 & 0.61 & 0.58 \\ \hline
\end{tabular}
\caption{Training accuracy for Adam and AdamW optimizers with different learning rates}
\label{tab:learningrates2}
\end{table}

From the results we can again see that the choice of optimizer doesn't have much effect in our cage, but it's clear that the learning rate of 0.0001 has produced best results for both of the optimizers.

\subsection{Batch Size}
Batch size is an improtant factor to consider during the training. Using a large enough batch size will allow the model to be trained faster than with a smaller batch size. Large batch size might also faciliate for some randomness in the optimization preventing the model from getting stuck in local minimas. We tested some possible batch sizes as shown in the table~\ref{tab:batchsizes}.
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Batch Size} & \textbf{Accuracy} \\ \hline
1 & 0.80 \\ \hline
8 & 0.59 \\ \hline
16 & 0.60 \\ \hline
32 & 0.56 \\ \hline
64 & 0.56 \\ \hline
128 & 0.54 \\ \hline
256 & 0.55 \\ \hline
\end{tabular}
\caption{Training accuracy for Adam optimizer with different batch sizes}
\label{tab:batchsizes}
\end{table}

From the results we can see that the batch size didn't have a massive effect on the accuracy. This is most likely since we train for so many epochs. There is a clear outlier in batch size of 1. This might be due to a lucky change in inital weights, but generally larger batch size a used with image classification.

\section{Data augmentation}
We tested the following model: 1x64 3x32 3x16 1x8 with all the datasets described in~\ref{sec:dataaug}. These results are shown in table~\ref{tab:dataaug}.
\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Dataset} & \textbf{Accuracy} \\ \hline
Base & 0.85 \\ \hline
Grayscale & 0.75 \\ \hline
Augmented & 0.87  \\ \hline
Augmented Grayscale & 0.83  \\ \hline
\end{tabular}
\caption{Training accuracy and loss for Adam optimizer with the specified datasets}
\label{tab:dataaug}
\end{table}

From the results we can conclude that our hypothesis about grayscale leaving more space for parameters to find features was not correct, since the grayscale datasets performed clearly worse then the color sets. We can also see that data augmentaion improved our accuracy a bit.

\section{The Final Model}
After all the experimentation we settled on the following model for our training:

Since we were limited on time and processing hardware we cannot be sure that this given model is the most performant model that we could have achieved. In our testing the model achieved a accuracy of: xx\% and was sufficiently accurate and fast when we tested it in the completed system.


\chapter{Conclusions}
\label{ch:conclusions}

%
% The bibliography, i.e the list of references
%
\newpage

\printbibliography[title=References]
\addcontentsline{toc}{chapter}{References}

\end{document}

